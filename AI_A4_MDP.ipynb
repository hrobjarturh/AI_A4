{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AI_A4_MDP.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hyv1iqLBoJnd"
      },
      "source": [
        "imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-YYCUDP7mp_O"
      },
      "source": [
        "import numpy as np\n",
        "import random\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pPcQ5dwdqSUI",
        "outputId": "bcb2f54a-db4d-49ee-b98a-dc7facea1507"
      },
      "source": [
        "class random_mdp():\n",
        "  def __init__(self):\n",
        "    \"\"\"\n",
        "    \"\"\"\n",
        "    self.floor_plan = [(i,j) for i in range(4) for j in range(3)]\n",
        "    self.V = [0. for i in range(12)]\n",
        "    self.possible_actions = ['L','R','U','D']\n",
        "    self.gamma = 1\n",
        "\n",
        "  def run(self):\n",
        "    print('floorplan layout: ') \n",
        "    for i in range(4):\n",
        "      print(self.floor_plan[i*3+0],self.floor_plan[i*3+1],self.floor_plan[i*3+2])\n",
        "\n",
        "    counter = 0\n",
        "    converge = True\n",
        "    while converge:\n",
        "      counter += 1\n",
        "      old_V = self.V[:]\n",
        "      self.perform_iteration() # perform an iteration\n",
        "      if old_V == self.V: # check for convergence\n",
        "        print('\\nIterations for convergence: ',counter)\n",
        "        converge = False\n",
        "\n",
        "    print('\\nFinal V values: ')\n",
        "    for i in range(4):\n",
        "      print(\"{:0.2f} {:0.2f} {:0.2f}\".format(self.V[i*3+0],self.V[i*3+1],self.V[i*3+2]))\n",
        "\n",
        "  def perform_iteration(self):\n",
        "    for state in self.floor_plan: # iterate through all states\n",
        "      possible_actions = []\n",
        "      index_current_state = self.floor_plan.index(state) # get the index of the current state\n",
        "      for action in self.possible_actions: # loop through each possible action\n",
        "        reward, next_state = self.get_next_state(action,state) # get the reward for moving and the next state\n",
        "        index_next_state = self.floor_plan.index(next_state) # get the index of the next state\n",
        "        v = reward + (self.gamma * self.V[index_next_state] *0.25) # calculate value usin bellmanns\n",
        "        possible_actions.append(v) # save the value\n",
        "      new_V = sum(possible_actions) / len(possible_actions) # get the mean of all possible actions\n",
        "      self.V[index_current_state] = new_V\n",
        "      \n",
        "  \n",
        "  def get_next_state(self, action, state):\n",
        "    old_state = list(state)\n",
        "    #print(old_state)\n",
        "    next_state = [3,2]\n",
        "    reward = 0\n",
        "    if old_state != [3,2]:\n",
        "      # check what action was taken and move to the correct state\n",
        "      if action == 'U': \n",
        "          next_state = [old_state[0] -1, old_state[1]]\n",
        "      if action == 'D':\n",
        "          next_state = [old_state[0] +1, old_state[1]]\n",
        "          if (old_state == [2,2]):\n",
        "            next_state = old_state\n",
        "      if action == 'L':\n",
        "          next_state = [old_state[0], old_state[1] -1]\n",
        "          if (old_state == [1,1]) or (old_state == [1,2]) or (old_state == [3,1]): # If there is no door, dont move\n",
        "            next_state = old_state\n",
        "      if action == 'R':\n",
        "          next_state = [old_state[0], old_state[1] +1]\n",
        "          if (old_state == [1,0]) or (old_state == [1,1]) or (old_state == [3,0]):\n",
        "            next_state = old_state\n",
        "\n",
        "      # correct illegal actions\n",
        "      if next_state[0] == -1 or next_state[0] == 4 or next_state[1] == -1 or next_state[1] == 3: # If going out of bounds, dont move\n",
        "        next_state = old_state\n",
        "\n",
        "      # give rewards\n",
        "      reward = -1\n",
        "      if (next_state == [2,0]) and (old_state != next_state): # Adding additional negative, unless already in room\n",
        "        reward += -7\n",
        "      if (next_state == [1,1]) and (old_state != next_state):\n",
        "        reward += -3\n",
        "      if (next_state == [2,2]) and (old_state != next_state):\n",
        "        reward += -2\n",
        "\n",
        "    return reward, tuple(next_state)\n",
        "     \n",
        "\n",
        "mdp = random_mdp()\n",
        "mdp.run()\n"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "floorplan layout: \n",
            "(0, 0) (0, 1) (0, 2)\n",
            "(1, 0) (1, 1) (1, 2)\n",
            "(2, 0) (2, 1) (2, 2)\n",
            "(3, 0) (3, 1) (3, 2)\n",
            "\n",
            "Iterations for convergence:  25\n",
            "\n",
            "Final V values: \n",
            "-1.54 -2.17 -1.44\n",
            "-3.38 -1.61 -1.93\n",
            "-1.82 -4.41 -1.60\n",
            "-3.52 -1.46 0.00\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vSs_LGiSt30e"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bamgaCWE5vNU",
        "outputId": "2c71dc08-a2a8-4c8e-acef-065dea961fee"
      },
      "source": [
        "class optimal_mdp():\n",
        "  def __init__(self):\n",
        "    \"\"\"\n",
        "    \"\"\"\n",
        "    self.floor_plan = [(i,j) for i in range(4) for j in range(3)]\n",
        "    self.V = [0. for i in range(12)]\n",
        "    self.possible_actions = ['L','R','U','D']\n",
        "    self.gamma = 1\n",
        "\n",
        "  def run(self):\n",
        "    print('floorplan layout: ') \n",
        "    for i in range(4):\n",
        "      print(self.floor_plan[i*3+0],self.floor_plan[i*3+1],self.floor_plan[i*3+2])\n",
        "\n",
        "    counter = 0\n",
        "    converge = True\n",
        "    while converge:\n",
        "      counter += 1\n",
        "      old_V = self.V[:]\n",
        "      self.get_optimal_value_function() # perform an iteration\n",
        "      if old_V == self.V: # check for convergence\n",
        "        print('\\nIterations for convergence to find optimal value function: ',counter)\n",
        "        converge = False\n",
        "\n",
        "    print('\\nOptimal V values: ')\n",
        "    for i in range(4):\n",
        "      print(\"{:0.2f} {:0.2f} {:0.2f}\".format(self.V[i*3+0],self.V[i*3+1],self.V[i*3+2]))\n",
        "    \n",
        "    optimal_policy = self.get_optimal_policy()\n",
        "    print('\\nOptimal Policy: ')\n",
        "    for i in range(4):\n",
        "      print(optimal_policy[i*3+0],optimal_policy[i*3+1],optimal_policy[i*3+2])\n",
        "\n",
        "  def get_optimal_policy(self):\n",
        "    policies = []\n",
        "    for state in self.floor_plan:\n",
        "      possible_actions = []\n",
        "      index_current_state = self.floor_plan.index(state) # get the index of the current state\n",
        "      for action in self.possible_actions: # loop through each possible action\n",
        "        reward, next_state = self.get_next_state(action,state) # get the reward for moving and the next state\n",
        "        index_next_state = self.floor_plan.index(next_state) # get the index of the next state\n",
        "        v = reward + (self.gamma * self.V[index_next_state]) # calculate value usin bellmanns\n",
        "        possible_actions.append(v) # save the value\n",
        "      new_V = max(possible_actions)\n",
        "      action_index = possible_actions.index(new_V)\n",
        "      policies.append(self.possible_actions[action_index])\n",
        "    return policies\n",
        "\n",
        "\n",
        "  def get_optimal_value_function(self):\n",
        "    for state in self.floor_plan: # iterate through all states\n",
        "      possible_actions = []\n",
        "      index_current_state = self.floor_plan.index(state) # get the index of the current state\n",
        "      for action in self.possible_actions: # loop through each possible action\n",
        "        reward, next_state = self.get_next_state(action,state) # get the reward for moving and the next state\n",
        "        index_next_state = self.floor_plan.index(next_state) # get the index of the next state\n",
        "        v = reward + (self.gamma * self.V[index_next_state] ) # calculate value usin bellmanns\n",
        "        possible_actions.append(v) # save the value\n",
        "      new_V = max(possible_actions) # Bellman Optimality Equation\n",
        "      self.V[index_current_state] = new_V\n",
        "      \n",
        "  \n",
        "  def get_next_state(self, action, state):\n",
        "    old_state = list(state)\n",
        "    #print(old_state)\n",
        "    next_state = [3,2]\n",
        "    reward = 0\n",
        "    if old_state != [3,2]:\n",
        "      # check what action was taken and move to the correct state\n",
        "      if action == 'U': \n",
        "          next_state = [old_state[0] -1, old_state[1]]\n",
        "      if action == 'D':\n",
        "          next_state = [old_state[0] +1, old_state[1]]\n",
        "          if (old_state == [2,2]):\n",
        "            next_state = old_state\n",
        "      if action == 'L':\n",
        "          next_state = [old_state[0], old_state[1] -1]\n",
        "          if (old_state == [1,1]) or (old_state == [1,2]) or (old_state == [3,1]): # If there is no door, dont move\n",
        "            next_state = old_state\n",
        "      if action == 'R':\n",
        "          next_state = [old_state[0], old_state[1] +1]\n",
        "          if (old_state == [1,0]) or (old_state == [1,1]) or (old_state == [3,0]):\n",
        "            next_state = old_state\n",
        "\n",
        "      # correct illegal actions\n",
        "      if next_state[0] == -1 or next_state[0] == 4 or next_state[1] == -1 or next_state[1] == 3: # If going out of bounds, dont move\n",
        "        next_state = old_state\n",
        "\n",
        "      # give rewards\n",
        "      reward = -1\n",
        "      if (next_state == [2,0]) and (old_state != next_state): # Adding additional negative, unless already in room\n",
        "        reward += -7\n",
        "      if (next_state == [1,1]) and (old_state != next_state):\n",
        "        reward += -3\n",
        "      if (next_state == [2,2]) and (old_state != next_state):\n",
        "        reward += -2\n",
        "\n",
        "    #print(action, self.current_state)\n",
        "    return reward, tuple(next_state)\n",
        "     \n",
        "\n",
        "mdp = optimal_mdp()\n",
        "mdp.run()\n"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "floorplan layout: \n",
            "(0, 0) (0, 1) (0, 2)\n",
            "(1, 0) (1, 1) (1, 2)\n",
            "(2, 0) (2, 1) (2, 2)\n",
            "(3, 0) (3, 1) (3, 2)\n",
            "\n",
            "Iterations for convergence to find optimal value function:  12\n",
            "\n",
            "Optimal V values: \n",
            "-8.00 -7.00 -7.00\n",
            "-9.00 -3.00 -6.00\n",
            "-3.00 -2.00 -3.00\n",
            "-11.00 -1.00 0.00\n",
            "\n",
            "Optimal Policy: \n",
            "R D D\n",
            "U D D\n",
            "R D L\n",
            "U R L\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1dG7aNC2NSA_"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}